{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import winsound as sd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024.0 576.0\n"
     ]
    }
   ],
   "source": [
    "# just cam\n",
    "\n",
    "# cap1 = cv2.VideoCapture(cv2.CAP_DSHOW + 0)\n",
    "# cap1.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "# cap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# c1_1 = cap1.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "# c1_2 = cap1.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "# print(c1_1, c1_2)\n",
    "\n",
    "cap2 = cv2.VideoCapture(cv2.CAP_DSHOW + 1)\n",
    "cap2.set(cv2.CAP_PROP_FRAME_WIDTH, 1024)\n",
    "cap2.set(cv2.CAP_PROP_FRAME_HEIGHT, 576)\n",
    "\n",
    "c2_1 = cap2.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c2_2 = cap2.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c2_1, c2_2)\n",
    "\n",
    "# cap3 = cv2.VideoCapture(cv2.CAP_DSHOW + 2)\n",
    "# cap3.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "# cap3.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# c3_1 = cap3.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "# c3_2 = cap3.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "# print(c3_1, c3_2)\n",
    "\n",
    "while True:\n",
    "    s2, img2 = cap2.read()\n",
    "    \n",
    "    if not s2:\n",
    "        print(\"camera is not found\")\n",
    "        continue\n",
    "    \n",
    "    img2_show = img2.copy()\n",
    "    img2_show = cv2.resize(img2_show, (960, 540))\n",
    "    \n",
    "    cv2.imshow('sign2', img2_show)\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280.0 720.0\n",
      "1280.0 720.0\n",
      "1280.0 720.0\n"
     ]
    }
   ],
   "source": [
    "# just cam\n",
    "\n",
    "cap1 = cv2.VideoCapture(cv2.CAP_DSHOW + 0)\n",
    "cap1.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "c1_1 = cap1.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c1_2 = cap1.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c1_1, c1_2)\n",
    "\n",
    "cap2 = cv2.VideoCapture(cv2.CAP_DSHOW + 1)\n",
    "cap2.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap2.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "c2_1 = cap2.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c2_2 = cap2.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c2_1, c2_2)\n",
    "\n",
    "cap3 = cv2.VideoCapture(cv2.CAP_DSHOW + 2)\n",
    "cap3.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap3.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "c3_1 = cap3.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c3_2 = cap3.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c3_1, c3_2)\n",
    "\n",
    "while True:\n",
    "    s1, img1 = cap1.read()\n",
    "    s2, img2 = cap2.read()\n",
    "    s3, img3 = cap3.read()\n",
    "    \n",
    "    if not s1 or not s2 or not s3:\n",
    "        print(\"camera is not found\")\n",
    "        continue\n",
    "    \n",
    "    img1_show = img1.copy()\n",
    "    img1_show = cv2.resize(img1_show, (960, 540))\n",
    "    \n",
    "    img2_show = img2.copy()\n",
    "    img2_show = cv2.resize(img2_show, (960, 540))\n",
    "    \n",
    "    img3_show = img3.copy()\n",
    "    img3_show = cv2.resize(img3_show, (960, 540))\n",
    "    \n",
    "    cv2.imshow('sign1', img1_show)\n",
    "    cv2.imshow('sign2', img2_show)\n",
    "    cv2.imshow('sign3', img3_show)\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cap3.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_save_dir = './cap_data'\n",
    "hangle_dir = './su_e_sample/'\n",
    "cnt = 0\n",
    "img_file_cnt = 72\n",
    "file_cnt_end = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv2.imread(hangle_dir + str(img_file_cnt % 5) +'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280.0 720.0\n",
      "1280.0 720.0\n",
      "1280.0 720.0\n",
      " now img count :  149"
     ]
    }
   ],
   "source": [
    "# cam capture\n",
    "\n",
    "cap1 = cv2.VideoCapture(cv2.CAP_DSHOW + 0)\n",
    "cap1.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "c1_1 = cap1.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c1_2 = cap1.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c1_1, c1_2)\n",
    "\n",
    "cap2 = cv2.VideoCapture(cv2.CAP_DSHOW + 1)\n",
    "cap2.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap2.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "c2_1 = cap2.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c2_2 = cap2.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c2_1, c2_2)\n",
    "\n",
    "cap3 = cv2.VideoCapture(cv2.CAP_DSHOW + 2)\n",
    "cap3.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap3.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "c3_1 = cap3.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "c3_2 = cap3.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(c3_1, c3_2)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        s1, img1 = cap1.read()\n",
    "        s2, img2 = cap2.read()\n",
    "        s3, img3 = cap3.read()\n",
    "        \n",
    "        if not s1 or not s2 or not s3:\n",
    "            print(\"camera is not found\")\n",
    "            continue\n",
    "        \n",
    "        img1_show = img1.copy()\n",
    "        img1_show = cv2.resize(img1_show, (960, 540))\n",
    "        \n",
    "        img1_show.flags.writeable = False\n",
    "        img1_show = cv2.cvtColor(img1_show, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img1_show)\n",
    "        \n",
    "        img1_show.flags.writeable = True\n",
    "        img1_show = cv2.cvtColor(img1_show, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    img1_show,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.DrawingSpec(\n",
    "                        color = (0,255,0),\n",
    "                        thickness = 1,\n",
    "                        circle_radius = 3),\n",
    "                    mp_drawing_styles.DrawingSpec(\n",
    "                        color = (255, 255, 255),\n",
    "                        thickness = 2,\n",
    "                        circle_radius = 1))\n",
    "        \n",
    "        cv2.imshow('sign_cap', img1_show)\n",
    "        \n",
    "        if cnt < 10:\n",
    "            str_1 = '2'\n",
    "        elif 9 < cnt < 18:\n",
    "            str_1 = '1'\n",
    "        else:\n",
    "            str_1 = '0'\n",
    "            \n",
    "        back_img = cv2.imread(hangle_dir + 'back_img.png')\n",
    "        cv2.putText(back_img, str(str_1), (0, 180), cv2.FONT_HERSHEY_SIMPLEX, 6, (0, 0, 0))\n",
    "        a = cv2.imread(hangle_dir + str(img_file_cnt % 3) +'.png')\n",
    "        a_tmp = np.concatenate((back_img, a), axis=1)\n",
    "        \n",
    "        cv2.imshow('1', a_tmp)\n",
    "        \n",
    "        if cnt > 22:\n",
    "            cap1_name = 'cam1_' + str(img_file_cnt).zfill(4) + '.png'\n",
    "            tmp_1 = os.path.join(img_save_dir, cap1_name)\n",
    "            \n",
    "            cap2_name = 'cam2_' + str(img_file_cnt).zfill(4) + '.png'\n",
    "            tmp_2 = os.path.join(img_save_dir, cap2_name)\n",
    "            \n",
    "            cap3_name = 'cam3_' + str(img_file_cnt).zfill(4) + '.png'\n",
    "            tmp_3 = os.path.join(img_save_dir, cap3_name)\n",
    "            \n",
    "            cv2.imwrite(tmp_1, img1)\n",
    "            cv2.imwrite(tmp_2, img2)\n",
    "            cv2.imwrite(tmp_3, img3)\n",
    "            sd.Beep(500, 100)\n",
    "            \n",
    "            if img_file_cnt == 100 or img_file_cnt == 200:\n",
    "                sd.Beep(1500, 100)\n",
    "                sd.Beep(1500, 300)\n",
    "            \n",
    "            cnt = 0\n",
    "            img_file_cnt += 1\n",
    "            \n",
    "        cnt += 1\n",
    "        \n",
    "        if img_file_cnt == file_cnt_end:\n",
    "            sd.Beep(1500, 100)\n",
    "            sd.Beep(1500, 100)\n",
    "            sd.Beep(1500, 300)\n",
    "            break\n",
    "        \n",
    "        \n",
    "        print(\"\\r now img count : \", img_file_cnt, end=\"\")\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cap3.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이 아래는 랜드마크 표시 관련 코드입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./cap_data/cam1_0000.png',\n",
       " './cap_data/cam1_0001.png',\n",
       " './cap_data/cam1_0002.png',\n",
       " './cap_data/cam1_0003.png',\n",
       " './cap_data/cam1_0004.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list = os.listdir('./cap_data')\n",
    "land_img_list = []\n",
    "\n",
    "for i in tmp_list:\n",
    "    if '.png' in i:\n",
    "        land_img_list.append('./cap_data/' + i)\n",
    "\n",
    "print(len(land_img_list))\n",
    "land_img_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18, 21):\n",
    "    os.makedirs('./cap_data_sort/real_word' + str(i).zfill(2), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:00<00:00, 477.16it/s]\n"
     ]
    }
   ],
   "source": [
    "land_img_list[0].split('/')[2].split('.')[0][-4:]\n",
    "\n",
    "for i in tqdm(land_img_list):\n",
    "    file_name = i.split('/')[2]\n",
    "    tmp_num = (int(file_name.split('.')[0][-4:]) % 3) + 18\n",
    "    shutil.copy2(i, './cap_data_sort/real_word' + str(tmp_num).zfill(2) + '/' + file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./cap_data_sort/real_word20/cam1_0002.png',\n",
       " './cap_data_sort/real_word20/cam1_0005.png',\n",
       " './cap_data_sort/real_word20/cam1_0008.png',\n",
       " './cap_data_sort/real_word20/cam1_0011.png']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_num = '20'\n",
    "\n",
    "cam_file_dir = './cap_data_sort/real_word' + folder_num + '/'\n",
    "\n",
    "cam_file_list = os.listdir(cam_file_dir)\n",
    "\n",
    "cam_file_dir_list = []\n",
    "\n",
    "for i in cam_file_list:\n",
    "    cam_file_dir_list.append(cam_file_dir + i)\n",
    "\n",
    "print(len(cam_file_dir_list))\n",
    "\n",
    "cam_file_dir_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:08, 16.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# One Hand\n",
    "# 이미지 파일의 경우을 사용하세요.: \n",
    "\n",
    "IMAGE_FILES = cam_file_dir_list\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "    for idx, file in tqdm(enumerate(IMAGE_FILES)):\n",
    "        # 이미지를 읽어 들이고, 보기 편하게 이미지를 좌우 반전합니다.\n",
    "        # image = cv2.flip(cv2.imread(file), 1)\n",
    "        image = cv2.imread(file)\n",
    "        # 작업 전에 BGR 이미지를 RGB로 변환합니다.\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # 손으로 프린트하고 이미지에 손 랜드마크를 그립니다.\n",
    "        # print('Handedness:', results.multi_handedness)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            continue\n",
    "        \n",
    "        result_keypoint = []\n",
    "        hand_keypoint = np.zeros((21, 3))\n",
    "        \n",
    "        for i in range(21):\n",
    "            hand_keypoint[i, 0] = results.multi_hand_landmarks[0].landmark[i].x\n",
    "            hand_keypoint[i, 1] = results.multi_hand_landmarks[0].landmark[i].y\n",
    "            hand_keypoint[i, 2] = results.multi_hand_landmarks[0].landmark[i].z\n",
    "            \n",
    "        for i in range(15):\n",
    "            result_keypoint.append(hand_keypoint)\n",
    "        \n",
    "        result_keypoint = np.array(result_keypoint)\n",
    "        \n",
    "        save_dir = './cap_data_npy/real_word' + folder_num + '/real_word' + folder_num + '_' + str(idx)\n",
    "        \n",
    "        np.save(save_dir + '.npy', result_keypoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:08, 18.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Two Hands\n",
    "# 이미지 파일의 경우을 사용하세요.:\n",
    "\n",
    "IMAGE_FILES = cam_file_dir_list\n",
    "\n",
    "compl_count = 0\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "    for idx, file in tqdm(enumerate(IMAGE_FILES)):\n",
    "        # 이미지를 읽어 들이고, 보기 편하게 이미지를 좌우 반전합니다.\n",
    "        # image = cv2.flip(cv2.imread(file), 1)\n",
    "        image = cv2.imread(file)\n",
    "        # 작업 전에 BGR 이미지를 RGB로 변환합니다.\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # 손으로 프린트하고 이미지에 손 랜드마크를 그립니다.\n",
    "        # print('Handedness:', results.multi_handedness)\n",
    "        if not results.multi_hand_landmarks:\n",
    "            continue\n",
    "        \n",
    "        result_keypoint = []\n",
    "        hand_keypoint = np.zeros((21, 3))\n",
    "        \n",
    "        both_hands = []\n",
    "        right_keypoints = []\n",
    "        left_keypoints = []\n",
    "        \n",
    "        right_keypoint = np.zeros((21, 3))\n",
    "        left_keypoint = np.zeros((21, 3))\n",
    "        \n",
    "        if len(results.multi_hand_landmarks) == 2:\n",
    "            for i in range(21):\n",
    "                if results.multi_hand_landmarks[0].landmark[0].x > results.multi_hand_landmarks[1].landmark[0].x:\n",
    "                    right_keypoint[i, 0] = results.multi_hand_landmarks[0].landmark[i].x\n",
    "                    right_keypoint[i, 1] = results.multi_hand_landmarks[0].landmark[i].y\n",
    "                    right_keypoint[i, 2] = results.multi_hand_landmarks[0].landmark[i].z\n",
    "\n",
    "                    left_keypoint[i, 0] = results.multi_hand_landmarks[1].landmark[i].x\n",
    "                    left_keypoint[i, 1] = results.multi_hand_landmarks[1].landmark[i].y\n",
    "                    left_keypoint[i, 2] = results.multi_hand_landmarks[1].landmark[i].z\n",
    "                else:\n",
    "                    right_keypoint[i, 0] = results.multi_hand_landmarks[1].landmark[i].x\n",
    "                    right_keypoint[i, 1] = results.multi_hand_landmarks[1].landmark[i].y\n",
    "                    right_keypoint[i, 2] = results.multi_hand_landmarks[1].landmark[i].z\n",
    "\n",
    "                    left_keypoint[i, 0] = results.multi_hand_landmarks[0].landmark[i].x\n",
    "                    left_keypoint[i, 1] = results.multi_hand_landmarks[0].landmark[i].y\n",
    "                    left_keypoint[i, 2] = results.multi_hand_landmarks[0].landmark[i].z\n",
    "            both_hand = np.concatenate((right_keypoint, left_keypoint))\n",
    "            \n",
    "            for i in range(30):\n",
    "                both_hands.append(both_hand)\n",
    "            \n",
    "            # right_keypoints = np.array(right_keypoints)\n",
    "            # left_keypoints = np.array(left_keypoints)\n",
    "            \n",
    "            both_hands = np.array(both_hands)        \n",
    "            \n",
    "            save_dir = './cap_data_npy/real_word' + folder_num + '/real_word' + folder_num + '_' + str(idx)\n",
    "            \n",
    "            np.save(save_dir + '.npy', both_hands)\n",
    "            \n",
    "            compl_count += 1\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "print(compl_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜드마크 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cap_data_npy/word01/word01_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnatsort\u001b[39;00m\n\u001b[0;32m      8\u001b[0m nnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m aa \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./cap_data_npy/word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnnn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnnn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_0.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m d \u001b[38;5;241m=\u001b[39m aa[\u001b[38;5;241m14\u001b[39m]\n\u001b[0;32m     14\u001b[0m img_read \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cap_data/cam1_0001.png\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\lib\\npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    388\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cap_data_npy/word01/word01_0.npy'"
     ]
    }
   ],
   "source": [
    "# one hand\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import natsort\n",
    "\n",
    "nnn = '01'\n",
    "\n",
    "aa = np.load('./cap_data_npy/word' + nnn + '/word' + nnn + '_0.npy')\n",
    "\n",
    "d = aa[14]\n",
    "\n",
    "img_read = cv2.imread('./cap_data/cam1_0001.png', cv2.IMREAD_COLOR)\n",
    "drawing_image = img_read.copy()\n",
    "\n",
    "d[:, 0] = d[:, 0] * 1280\n",
    "d[:, 1] = d[:, 1] * 720\n",
    "d_mean = d.astype(int)\n",
    "\n",
    "for ar in d_mean:\n",
    "    cv2.circle(drawing_image, (ar[0], ar[1]), 5, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "while True:\n",
    "    cv2.imshow('111', drawing_image)\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two hand\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import natsort\n",
    "\n",
    "nnn = '20'\n",
    "\n",
    "aa = np.load('./cap_data_npy/real_word' + nnn + '/real_word' + nnn + '_0.npy')\n",
    "# aa = np.load(r'C:\\Users\\user\\Documents\\1_deep\\real_word15_49.npy')\n",
    "\n",
    "d = aa[0]\n",
    "\n",
    "right_h = d[:21, :]\n",
    "left_h = d[21:, :]\n",
    "\n",
    "# print(right_h.shape, left_h.shape)\n",
    "\n",
    "right_h[:, 0] = right_h[:, 0] * 1280\n",
    "right_h[:, 1] = right_h[:, 1] * 720\n",
    "right_h_dot = right_h.astype(int)\n",
    "\n",
    "left_h[:, 0] = left_h[:, 0] * 1280\n",
    "left_h[:, 1] = left_h[:, 1] * 720\n",
    "left_h_dot = left_h.astype(int)\n",
    "\n",
    "img_read = cv2.imread('./cap_data/cam1_0002.png', cv2.IMREAD_COLOR)\n",
    "# img_read = cv2.imread(r'C:\\Users\\user\\Documents\\1_deep\\cam1_0297.png')\n",
    "drawing_image = img_read.copy()\n",
    "\n",
    "for a in right_h_dot:\n",
    "    cv2.circle(drawing_image, (a[0], a[1]), 5, (0, 255, 0), -1)\n",
    "    \n",
    "for a in left_h_dot:\n",
    "    cv2.circle(drawing_image, (a[0], a[1]), 5, (0, 255, 0), -1)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow('1', drawing_image)\n",
    "    \n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "# d[:, 0] = d[:, 0] * 1280\n",
    "# d[:, 1] = d[:, 1] * 720\n",
    "# d_mean = d.astype(int)\n",
    "\n",
    "# for ar in d_mean:\n",
    "#     cv2.circle(drawing_image, (ar[0], ar[1]), 5, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     cv2.imshow('111', drawing_image)\n",
    "    \n",
    "#     if cv2.waitKey(5) & 0xFF == 27:\n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cap_data_dir = './cap_data/'\n",
    "\n",
    "cap_list = os.listdir(cap_data_dir)\n",
    "cap_list2 = []\n",
    "\n",
    "for i in cap_list:\n",
    "    ttt_mp = os.path.join(cap_data_dir, i)\n",
    "    cap_list2.append(ttt_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# 이미지 파일의 경우을 사용하세요.:\n",
    "IMAGE_FILES = cap_list2\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    # 이미지를 읽어 들이고, 보기 편하게 이미지를 좌우 반전합니다.\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # 작업 전에 BGR 이미지를 RGB로 변환합니다.\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 손으로 프린트하고 이미지에 손 랜드마크를 그립니다.\n",
    "    # print('Handedness:', results.multi_handedness)\n",
    "    if not results.multi_hand_landmarks:\n",
    "      continue\n",
    "    image_height, image_width, _ = image.shape\n",
    "    annotated_image = image.copy()\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "    #   print('hand_landmarks:', hand_landmarks)\n",
    "    #   print(\n",
    "    #       f'Index finger tip coordinates: (',\n",
    "    #       f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "    #       f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_height})'\n",
    "    #   )\n",
    "      mp_drawing.draw_landmarks(\n",
    "          annotated_image,\n",
    "          hand_landmarks,\n",
    "          mp_hands.HAND_CONNECTIONS,\n",
    "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "          mp_drawing_styles.get_default_hand_connections_style())\n",
    "    cv2.imwrite(\n",
    "        './tmp/1006_annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
